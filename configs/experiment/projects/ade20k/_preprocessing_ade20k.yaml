# @package _global_

dataset:
  # Minimal preprocessing for ADE20K images + semantic masks.
  eval_transforms:
    00_rename_fields:
      _target_: ocl.transforms.Map
      transform:
        _target_: ocl.preprocessing.RenameFields
        mapping:
          segmentation_map: mask
      fields:
        - segmentation_map
      batch_transform: false
    03_preprocessing:
      _target_: ocl.transforms.SimpleTransform
      transforms:
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
        mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.IntegerToOneHotMask
              output_axis: -4
              max_instances: 150
              ignore_typical_background: false
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 128
      batch_transform: false
  train_transforms:
    00_rename_fields:
      _target_: ocl.transforms.Map
      transform:
        _target_: ocl.preprocessing.RenameFields
        mapping:
          segmentation_map: mask
      fields:
        - segmentation_map
      batch_transform: false
    03_preprocessing:
      _target_: ocl.transforms.SimpleTransform
      transforms:
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.RandomHorizontalFlip
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
      batch_transform: false
