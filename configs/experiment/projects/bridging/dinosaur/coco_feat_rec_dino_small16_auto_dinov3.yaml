# @package _global_
# ViT feature reconstruction on COCO using an autoregressive decoder (SLATE style)
defaults:
  - /experiment/projects/bridging/dinosaur/_base_feature_recon
  - /dataset: coco
  - /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop
  - /experiment/projects/bridging/dinosaur/_metrics_coco
  - _self_

# The following parameters assume training on 4 GPUs, leading to an effective batch size of 32.
trainer:
  devices: 1
  max_steps: 500000
  max_epochs:
  gradient_clip_val: 1.0

dataset:
  num_workers: 4
  batch_size: 16

experiment:
  # Match DINOv3 ViT-7B patch token embedding dimension
  input_feature_dim: 4096
  hierarchy:
    depth: 3
    child_per_parent: 2
    child_loss_weight: 0.3
    child2_loss_weight: 0.2

models:
  conditioning:
    _target_: routed.ocl.conditioning.RandomConditioning
    n_slots: 7
    object_dim: 256

    batch_size_path: input.batch_size
  feature_extractor:
    model_name: vit_7b_patch16_dinov3.lvd1689m
    pretrained: true
    freeze: true

  object_decoder:
    _target_: routed.ocl.decoding.AutoregressivePatchDecoder
    decoder_cond_dim: ${.output_dim}
    use_input_transform: true
    use_decoder_masks: true
    resize_mode: bicubic
    decoder:
      _target_: ocl.neural_networks.build_transformer_decoder
      _partial_: true
      n_layers: 4
      n_heads: 8
      return_attention_weights: true
    masks_path: perceptual_grouping.feature_attributions
    object_features_path: perceptual_grouping.objects
  hierarchical_refine:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: perceptual_grouping.objects
    parent_masks_path: object_decoder.masks
    child_per_parent: ${experiment.hierarchy.child_per_parent}
    slot_dim: ${models.conditioning.object_dim}
    sa_iters: 3
    feature_dim: null
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 1.0

  object_decoder_child:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: ${models.perceptual_grouping.object_dim}
    output_dim: ${experiment.input_feature_dim}
    num_patches: 196
    resize_mode: bicubic
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine.child_objects
    gating_mask_path: hierarchical_refine.child_gating_masks
    image_path: input.image

  hierarchical_refine_l2:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: hierarchical_refine.child_objects
    parent_masks_path: object_decoder_child.masks
    child_per_parent: ${experiment.hierarchy.child_per_parent}
    slot_dim: ${models.conditioning.object_dim}
    sa_iters: 3
    feature_dim: null
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 1.0

  object_decoder_grandchild:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: ${models.perceptual_grouping.object_dim}
    output_dim: ${experiment.input_feature_dim}
    num_patches: 196
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine_l2.child_objects
    gating_mask_path: hierarchical_refine_l2.child_gating_masks
    image_path: input.image
    resize_mode: bicubic

  child_masks_as_image:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_child.masks
    size: 128
    resize_mode: bicubic
    patch_mode: true

  grandchild_masks_as_image:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_grandchild.masks
    size: 128
    resize_mode: bicubic
    patch_mode: true

losses:
  mse_child:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    weight: "${eval_lambda:'lambda d, w: 0.0 if d < 2 else w', ${experiment.hierarchy.depth}, ${experiment.hierarchy.child_loss_weight}}"
    input_path: object_decoder_child.reconstruction
    target_path: object_decoder.target
  mse_child2:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    weight: "${eval_lambda:'lambda d, w: 0.0 if d < 3 else w', ${experiment.hierarchy.depth}, ${experiment.hierarchy.child2_loss_weight}}"
    input_path: object_decoder_grandchild.reconstruction
    target_path: object_decoder.target

visualizations:
  child_masks:
    _target_: routed.ocl.visualizations.Mask
    mask_path: child_masks_as_image
  grandchild_masks:
    _target_: routed.ocl.visualizations.Mask
    mask_path: grandchild_masks_as_image
