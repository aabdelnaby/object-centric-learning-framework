# @package _global_
# ViT feature reconstruction on MOVI-C.
defaults:
  - /experiment/projects/bridging/dinosaur/_base_feature_recon
  - /dataset: movi_c_image
  - /experiment/projects/bridging/dinosaur/_preprocessing_movi_dino_feature_recon
  - /experiment/projects/bridging/dinosaur/_metrics_clevr_patch
  - _self_

# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.
trainer:
  devices: 4
  max_steps: 500000

dataset:
  num_workers: 4
  batch_size: 8

experiment:
  # Match ViT-S/8 embed dimension (not ADE20K classes)
  input_feature_dim: 384
  hierarchy:
    depth: 3                 # 1 = parent only, 2 = parent+child
    child_per_parent: 2      # K2 children per parent
    child_loss_weight: 0.3   # set to 0.0 to disable child optimization
    child2_loss_weight: 0.2  # weight for level-3 (grandchild) reconstruction

models:
  conditioning:
    _target_: routed.ocl.conditioning.RandomConditioning
    n_slots: 4
    object_dim: 128

    batch_size_path: input.batch_size
  feature_extractor:
    _target_: routed.ocl.feature_extractors.TimmFeatureExtractor
    model_name: vit_small_patch8_224_dino
    pretrained: true
    freeze: true
    feature_level: 12  # Use the final layer features

  # Override positional embedding to project from ViT token dim (384) -> slot feature dim
  perceptual_grouping:
    positional_embedding:
      _target_: ocl.neural_networks.wrappers.Sequential
      _args_:
        - _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed
        - _target_: ocl.neural_networks.build_two_layer_mlp
          input_dim: 384          # ViT-S/8 embed dim
          output_dim: ${....feature_dim}
          hidden_dim: 384
          initial_layer_norm: true

  object_decoder:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: ${models.perceptual_grouping.object_dim}
    output_dim: ${experiment.input_feature_dim}  # Now 384
    num_patches: 784
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: perceptual_grouping.objects

  # Build child slots per parent 
  hierarchical_refine:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: perceptual_grouping.objects
    parent_masks_path: object_decoder.masks
    child_per_parent: ${experiment.hierarchy.child_per_parent}
    slot_dim: ${models.conditioning.object_dim}
    sa_iters: 3
    feature_dim: null   # infer from tokens at runtime (matches ViT embed dim)
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 1.0

  # Decode children and gate by parent regions
  object_decoder_child:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: ${models.perceptual_grouping.object_dim}
    output_dim: ${experiment.input_feature_dim}  # Now 384
    num_patches: 784
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine.child_objects
    gating_mask_path: hierarchical_refine.child_gating_masks
    image_path: input.image

  # Level 3: decompose each child into grandchildren if desired
  hierarchical_refine_l2:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: hierarchical_refine.child_objects
    parent_masks_path: object_decoder_child.masks
    child_per_parent: ${experiment.hierarchy.child_per_parent}
    slot_dim: ${models.conditioning.object_dim}
    sa_iters: 3
    feature_dim: null   # infer from tokens at runtime (matches ViT embed dim)
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 1.0

  object_decoder_grandchild:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: ${models.perceptual_grouping.object_dim}
    output_dim: ${experiment.input_feature_dim}  # Now 384
    num_patches: 784
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine_l2.child_objects
    gating_mask_path: hierarchical_refine_l2.child_gating_masks
    image_path: input.image

  masks_as_image:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true

  # Resized child masks for visualization
  child_masks_as_image:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_child.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true

  # Resized grandchild masks for visualization
  grandchild_masks_as_image:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_grandchild.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true

visualizations:
  child_masks:
    _target_: routed.ocl.visualizations.Mask
    mask_path: child_masks_as_image
  grandchild_masks:
    _target_: routed.ocl.visualizations.Mask
    mask_path: grandchild_masks_as_image

losses:
  mse_child:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    weight: "${eval_lambda:'lambda d, w: 0.0 if d < 2 else w', ${experiment.hierarchy.depth}, ${experiment.hierarchy.child_loss_weight}}"
    input_path: object_decoder_child.reconstruction
    target_path: object_decoder.target
  mse_child2:
    _target_: routed.ocl.losses.ReconstructionLoss
    loss_type: mse
    weight: "${eval_lambda:'lambda d, w: 0.0 if d < 3 else w', ${experiment.hierarchy.depth}, ${experiment.hierarchy.child2_loss_weight}}"
    input_path: object_decoder_grandchild.reconstruction
    target_path: object_decoder.target
