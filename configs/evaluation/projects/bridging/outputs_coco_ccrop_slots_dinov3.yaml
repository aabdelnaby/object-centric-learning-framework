# @package _global_
# Save model predictions on COCO
defaults:
  - /evaluation_config
  - /evaluation/projects/bridging/_base_metrics
  - /evaluation/projects/bridging/_preprocessing_coco
  - /dataset: coco
  - _self_

# dataset:
#   shuffle_train: false

#   train_transforms:
#     02a_preprocessing:
#       _target_: ocl.transforms.Map
#       transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
#       fields:
#         - image
#       batch_transform: false
#   eval_transforms:
#     02a_preprocessing:
#       _target_: ocl.transforms.Map
#       transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
#       fields:
#         - image
#       batch_transform: false
eval_batch_size: 1

eval_train: true
save_outputs: true
skip_metrics: false
n_samples_to_store: 500
outputs_dirname: dinov3_huge

outputs_to_store:
  - perceptual_grouping.objects
  - input.instance_mask
  - parent_masks_128
  - child_masks_128
  - grandchild_masks_128
  - input.orig_image
  # - input.orig_image
  - masks_resized


modules:
  hierarchical_refine:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: perceptual_grouping.objects
    parent_masks_path: object_decoder.masks
    child_per_parent: 2
    slot_dim: 256
    sa_iters: 3
    feature_dim: null
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 0.5
  object_decoder_child:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: 256
    output_dim: 4096
    num_patches: 196
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine.child_objects
    gating_mask_path: hierarchical_refine.child_gating_masks
    image_path: input.image
  hierarchical_refine_l2:
    _target_: routed.ocl.hierarchy.HierarchicalSlots
    tokens_path: feature_extractor.features
    parent_slots_path: hierarchical_refine.child_objects
    parent_masks_path: object_decoder_child.masks
    child_per_parent: 2
    slot_dim: 256
    sa_iters: 3
    feature_dim: null
    use_sa_attention_for_gating: true
    use_straight_through_gumbel: true
    gumbel_temperature: 0.5
  object_decoder_grandchild:
    _target_: routed.ocl.decoding.PatchDecoder
    object_dim: 256
    output_dim: 4096
    num_patches: 196
    decoder:
      _target_: ocl.neural_networks.build_mlp
      _partial_: true
      features: [1024, 1024, 1024]
    object_features_path: hierarchical_refine_l2.child_objects
    gating_mask_path: hierarchical_refine_l2.child_gating_masks
    image_path: input.image
  parent_masks_128:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true
  child_masks_128:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_child.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true
  grandchild_masks_128:
    _target_: routed.ocl.utils.resizing.Resize
    input_path: object_decoder_grandchild.masks
    size: 128
    resize_mode: bilinear
    patch_mode: true

dataset:
  shuffle_train: false
  train_transforms:
    02a_preprocessing:
      _target_: ocl.transforms.Map
      transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
      fields:
        - image
      batch_transform: false
    03a_preprocessing:
      _target_: ocl.transforms.Map
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.InstanceMasksToDenseMasks
          - _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask
          - _target_: ocl.preprocessing.AddEmptyMasks
            mask_keys:
              - instance_mask
              - segmentation_mask
          - _target_: ocl.preprocessing.DropEntries
            keys:
              - instance_category
      fields:
        - image
        - instance_mask
        - instance_category
      batch_transform: false
    03b_preprocessing:
      _target_: ocl.transforms.SimpleTransform
      transforms:
        orig_image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: ocl.preprocessing.OrigCenterCrop
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.CenterCrop
              size: 224
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
        instance_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 224
            - _target_: torchvision.transforms.CenterCrop
              size: 224
        segmentation_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 224
            - _target_: torchvision.transforms.CenterCrop
              size: 224
      batch_transform: false
  eval_transforms:
    02a_preprocessing:
      _target_: ocl.transforms.Map
      transform: "${lambda_fn:'lambda data: {\"orig_image\": data[\"image\"], **data}'}"
      fields:
        - image
      batch_transform: false
    03a_preprocessing:
      _target_: ocl.transforms.Map
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.InstanceMasksToDenseMasks
          - _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask
          - _target_: ocl.preprocessing.AddEmptyMasks
            mask_keys:
              - instance_mask
              - segmentation_mask
          - _target_: ocl.preprocessing.DropEntries
            keys:
              - instance_category
      fields:
        - image
        - instance_mask
        - instance_category
      batch_transform: false
    03b_preprocessing:
      _target_: ocl.transforms.SimpleTransform
      transforms:
        orig_image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: ocl.preprocessing.OrigCenterCrop
        image:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.ToTensor
            - _target_: torchvision.transforms.Resize
              size: 224
              interpolation: ${torchvision_interpolation_mode:BICUBIC}
            - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"
            - _target_: torchvision.transforms.CenterCrop
              size: 224
            - _target_: torchvision.transforms.Normalize
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
        instance_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 224
            - _target_: torchvision.transforms.CenterCrop
              size: 224
        segmentation_mask:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: ocl.preprocessing.DenseMaskToTensor
            - _target_: ocl.preprocessing.ResizeNearestExact
              size: 224
            - _target_: torchvision.transforms.CenterCrop
              size: 224
